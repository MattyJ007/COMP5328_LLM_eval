{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "FASHION_5_TRANSITION_MATRIX=[[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]\n",
    "FASHION_6_TRANSITION_MATRIX=[[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]]\n",
    "CIFAR_TRANSITION_MATRIX=[[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]] #dummy matrix\n",
    "\n",
    "fashion5 = './data/FashionMNIST0.5.npz'\n",
    "fashion6 = './data/FashionMNIST0.6.npz'\n",
    "cifar = './data/CIFAR.npz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RF on each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RF on ./data/FashionMNIST0.5.npz\n",
      "RF Best Validation Accuracy:  0.47711111111111115\n",
      "Best Parameters:  {'max_depth': 10, 'n_estimators': 200}\n",
      "RF Test Accuracy:  0.925\n",
      "Estimated Transition Matrix: \n",
      " [[0.605      0.16883333 0.22616667]\n",
      " [0.2215     0.59233333 0.18616667]\n",
      " [0.16433333 0.23083333 0.60483333]]\n",
      "Actual Transition Matrix: \n",
      " [[0.5 0.2 0.3]\n",
      " [0.3 0.5 0.2]\n",
      " [0.2 0.3 0.5]]\n"
     ]
    }
   ],
   "source": [
    "classifyRf(fashion5, FASHION_5_TRANSITION_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RF on ./data/FashionMNIST0.6.npz\n",
      "RF Best Validation Accuracy:  0.3868888888888889\n",
      "Best Parameters:  {'max_depth': 10, 'n_estimators': 200}\n",
      "RF Test Accuracy:  0.862\n",
      "Estimated Transition Matrix: \n",
      " [[0.60816667 0.19966667 0.19216667]\n",
      " [0.1485     0.63366667 0.21783333]\n",
      " [0.1245     0.18566667 0.68983333]]\n",
      "Actual Transition Matrix: \n",
      " [[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "classifyRf(fashion6, FASHION_6_TRANSITION_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RF on ./data/CIFAR.npz\n",
      "RF Best Validation Accuracy:  0.3648666666666667\n",
      "Best Parameters:  {'max_depth': 10, 'n_estimators': 200}\n",
      "RF Test Accuracy:  0.5046666666666667\n",
      "Estimated Transition Matrix: \n",
      " [[0.9684 0.011  0.0206]\n",
      " [0.1406 0.829  0.0304]\n",
      " [0.137  0.03   0.833 ]]\n",
      "Actual Transition Matrix: \n",
      " [[0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4]]\n"
     ]
    }
   ],
   "source": [
    "classifyRf(cifar, CIFAR_TRANSITION_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(file_path):\n",
    "  dataset = np.load(file_path)\n",
    "  xtr_val = dataset['Xtr']\n",
    "  str_val = dataset['Str']\n",
    "  xts = dataset['Xts']\n",
    "  yts = dataset['Yts']\n",
    "\n",
    "  #convert to greyscale\n",
    "  if xtr_val.shape[-1] == 3:\n",
    "        xtr_val = rgb2gray(xtr_val)\n",
    "        xts = rgb2gray(xts)\n",
    "\n",
    "  #flatten the image into 1D array\n",
    "  xtr_val = xtr_val.reshape(xtr_val.shape[0], -1)\n",
    "  xts = xts.reshape(xts.shape[0], -1)\n",
    "\n",
    "  #normalize the data\n",
    "  xtr_val = xtr_val.astype('float32') / 255.\n",
    "  xts = xts.astype('float32') / 255.\n",
    "\n",
    "  return xtr_val, str_val, xts, yts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyRf(filename, transition_matrix, count=2): #count should be 10 according to 2.1.1\n",
    "\n",
    "    print(\"Running RF on\", filename)\n",
    "\n",
    "    xtr_val, str_val, xts, yts = getData(filename)\n",
    "\n",
    "    # Preprocessing\n",
    "    num_classes = len(np.unique(str_val))\n",
    "    str_val_categorical = to_categorical(str_val, num_classes)\n",
    "\n",
    "    # Compute sample weights based on the inverse of the transition matrix\n",
    "    # Assuming the transition_matrix is of the shape (num_classes, num_classes)\n",
    "    # and the element T[i, j] is the probability of class i being mislabeled as class j.\n",
    "    # Ensure transition_matrix is a numpy array\n",
    "    transition_matrix = np.array(transition_matrix)\n",
    "\n",
    "    # Calculate sample weights using the diagonal of the transition matrix\n",
    "    sample_weights = np.array([1 / transition_matrix[j][j] for j in str_val])\n",
    "\n",
    "    \n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [200],\n",
    "        'max_depth': [10]\n",
    "        #'n_estimators': [50, 100, 200],\n",
    "        #'max_depth': [10, 20, None]\n",
    "    }\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=count, scoring='accuracy')\n",
    "\n",
    "    grid_search.fit(xtr_val, np.argmax(str_val_categorical, axis=1), sample_weight=sample_weights)\n",
    "\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(\"RF Best Validation Accuracy: \", best_score)\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "    best_rf.fit(xtr_val, np.argmax(str_val_categorical, axis=1), sample_weight=sample_weights)\n",
    "    \n",
    "    yts_pred = best_rf.predict(xts)\n",
    "    test_accuracy = accuracy_score(yts, yts_pred)\n",
    "    print(\"RF Test Accuracy: \", test_accuracy)\n",
    "\n",
    "    str_val_pred = best_rf.predict(xtr_val)\n",
    "    \n",
    "    # Construct confusion matrix using predicted and noisy labels\n",
    "    conf_matrix = confusion_matrix(np.argmax(str_val_categorical, axis=1), str_val_pred)\n",
    "\n",
    "    # Normalize the confusion matrix to estimate the transition matrix\n",
    "    estimated_transition_matrix = normalize(conf_matrix, axis=1, norm='l1')\n",
    "    print(\"Estimated Transition Matrix: \\n\", estimated_transition_matrix)\n",
    "    print(\"Actual Transition Matrix: \\n\", transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(transition_matrix):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred_adjusted = K.dot(y_pred, K.constant(transition_matrix, dtype='float32'))\n",
    "        return K.categorical_crossentropy(y_true, y_pred_adjusted)\n",
    "    return loss\n",
    "\n",
    "def build_cnn(input_shape, num_classes, transition_matrix):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss=custom_loss(transition_matrix), optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocess_data(x, y, num_classes):\n",
    "    x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1).astype('float32') / 255\n",
    "    y = to_categorical(y, num_classes)\n",
    "    return x, y\n",
    "\n",
    "def evaluate_model(cnn, x_test, y_test):\n",
    "    predictions = cnn.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test, axis=1)\n",
    "    return accuracy_score(true_classes, predicted_classes)\n",
    "\n",
    "def classifyCnn(filename, transition_matrix, count=1):\n",
    "    xtr_val, ytr_val, xts, yts = getData(filename)\n",
    "    num_classes = len(np.unique(ytr_val))\n",
    "\n",
    "    xtr_val, ytr_val = preprocess_data(xtr_val, ytr_val, num_classes)\n",
    "    xts, yts = preprocess_data(xts, yts, num_classes)\n",
    "\n",
    "    accuracies = []\n",
    "    for _ in range(count):\n",
    "        cnn = build_cnn(xtr_val.shape[1:], num_classes, transition_matrix)\n",
    "        cnn.fit(xtr_val, ytr_val, epochs=10, batch_size=64, verbose=0)\n",
    "        acc = evaluate_model(cnn, xts, yts)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    print(\"CNN Average Validation Accuracy: \", np.mean(accuracies))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1024,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arche\\OneDrive\\Documents\\GitHub\\COMP5328_noise_classifier\\Assignment2.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m   plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m   plt\u001b[39m.\u001b[39mclose()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m plot_examples(\u001b[39m'\u001b[39;49m\u001b[39mCIFAR Training Data\u001b[39;49m\u001b[39m'\u001b[39;49m, cifar_xtr, cifar_str, \u001b[39mlen\u001b[39;49m(CIFAR_CLASS_NAMES), EXAMPLES, CIFAR_CATEGORY_LABELS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m plot_examples(\u001b[39m'\u001b[39m\u001b[39mCIFAR Test Data\u001b[39m\u001b[39m'\u001b[39m, cifar_xts, cifar_yts, \u001b[39mlen\u001b[39m(CIFAR_CLASS_NAMES), EXAMPLES, CIFAR_CATEGORY_LABELS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m plot_examples(\u001b[39m'\u001b[39m\u001b[39mFashion 0.5 Training Data\u001b[39m\u001b[39m'\u001b[39m, fashion5_xtr, fashion5_str, \u001b[39mlen\u001b[39m(FASHION_CLASS_NAMES), EXAMPLES, FASHION_CATEGORY_LABELS)\n",
      "\u001b[1;32mc:\\Users\\arche\\OneDrive\\Documents\\GitHub\\COMP5328_noise_classifier\\Assignment2.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(examples):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   plt\u001b[39m.\u001b[39msubplot(categories, examples, count),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m   plt\u001b[39m.\u001b[39;49mimshow(data_set[categoryIndeces[\u001b[39m0\u001b[39;49m][j]], cmap \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mbinary\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m   plt\u001b[39m.\u001b[39mtitle(category_labels[\u001b[39mstr\u001b[39m(data_noisy_labels[categoryIndeces[\u001b[39m0\u001b[39m][j]])]), plt\u001b[39m.\u001b[39mxticks([]), plt\u001b[39m.\u001b[39myticks([])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arche/OneDrive/Documents/GitHub/COMP5328_noise_classifier/Assignment2.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[0;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[0;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[0;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[0;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[0;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[0;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[0;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2703\u001b[0m     sci(__ret)\n\u001b[0;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\__init__.py:1461\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1460\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1463\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1464\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1465\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5656\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[0;32m   5657\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[0;32m   5658\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[0;32m   5659\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[0;32m   5660\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5661\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5663\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5664\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5665\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5666\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[1;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (1024,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXAMPLES=10\n",
    "# An ordered list of the CIFAR class names\n",
    "CIFAR_CLASS_NAMES = [\"0. Plane\", \"1. Car\", \"2. Cat\"]\n",
    "CIFAR_CATEGORY_LABELS = dict(zip(map(str, list(range(3))), CIFAR_CLASS_NAMES))\n",
    "\n",
    "FASHION_CLASS_NAMES = [\"0. T-shirt\", \"1. Pants\", \"2. Dress\"]\n",
    "FASHION_CATEGORY_LABELS = dict(zip(map(str, list(range(3))), FASHION_CLASS_NAMES))\n",
    "\n",
    "cifar_xtr, cifar_str, cifar_xts, cifar_yts = getData('./data/CIFAR.npz')\n",
    "# print('CIFAR\\n---------------')\n",
    "# print('Training samples:\\n', pd.DataFrame(cifar_str).value_counts())\n",
    "# print('Test samples:\\n', pd.DataFrame(cifar_yts).value_counts())\n",
    "\n",
    "fashion5_xtr, fashion5_str, fashion5_xts, fashion5_yts = getData('./data/FashionMNIST0.5.npz')\n",
    "# print('Fashion 0.5\\n---------------')\n",
    "# print('Training samples:\\n', pd.DataFrame(fashion5_str).value_counts())\n",
    "# print('Test samples:\\n', pd.DataFrame(fashion5_yts).value_counts())\n",
    "\n",
    "fashion6_xtr, fashion6_str, fashion6_xts, fashion6_yts = getData('./data/FashionMNIST0.6.npz')\n",
    "# print('Fashion 0.6\\n---------------')\n",
    "# print('Training samples:\\n', pd.DataFrame(fashion6_str).value_counts())\n",
    "# print('Test samples:\\n', pd.DataFrame(fashion6_yts).value_counts())\n",
    "\n",
    "def plot_examples(title, data_set, data_noisy_labels, categories, examples, category_labels):\n",
    "  fig = plt.figure(figsize=(examples, categories))  # Added a figure instance with a specified size\n",
    "  count = 1\n",
    "  for i in range(categories):\n",
    "    categoryIndeces = np.where(data_noisy_labels == i)\n",
    "    for j in range(examples):\n",
    "      plt.subplot(categories, examples, count),\n",
    "      plt.imshow(data_set[categoryIndeces[0][j]], cmap = 'binary')\n",
    "      plt.title(category_labels[str(data_noisy_labels[categoryIndeces[0][j]])]), plt.xticks([]), plt.yticks([])\n",
    "      count += 1\n",
    "  \n",
    "  fig.suptitle(title, fontsize=16)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "plot_examples('CIFAR Training Data', cifar_xtr, cifar_str, len(CIFAR_CLASS_NAMES), EXAMPLES, CIFAR_CATEGORY_LABELS)\n",
    "plot_examples('CIFAR Test Data', cifar_xts, cifar_yts, len(CIFAR_CLASS_NAMES), EXAMPLES, CIFAR_CATEGORY_LABELS)\n",
    "\n",
    "plot_examples('Fashion 0.5 Training Data', fashion5_xtr, fashion5_str, len(FASHION_CLASS_NAMES), EXAMPLES, FASHION_CATEGORY_LABELS)\n",
    "plot_examples('Fashion 0.5 Test Data', fashion5_xts, fashion5_yts, len(FASHION_CLASS_NAMES), EXAMPLES, FASHION_CATEGORY_LABELS)\n",
    "\n",
    "plot_examples('Fashion 0.6 Training Data', fashion6_xtr, fashion6_str, len(FASHION_CLASS_NAMES), EXAMPLES, FASHION_CATEGORY_LABELS)\n",
    "plot_examples('Fashion 0.6 Test Data', fashion6_xts, fashion6_yts, len(FASHION_CLASS_NAMES), EXAMPLES, FASHION_CATEGORY_LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
